| Model    | Year | Parameters    | Pretraining Data                       | Architecture | Top-1 Accuracy          | Notable Features                          | Important Points                                          |
|----------|------|---------------|---------------------------------------|--------------|-------------------------|-------------------------------------------|-----------------------------------------------------------|
| GPT      | 2018 | 110 million   | Web text                              | Transformer  | N/A                     | First large-scale Transformer model       | Groundbreaking model, established baseline for successors. |
| BERT     | 2018 | 340 million   | Books, Wikipedia                      | Transformer  | 76.4% (LM), 93.5% (LM + FT) | Bidirectional Transformer, invented masked language modeling | Revolutionized NLP with bidirectional context understanding. |
| GPT-2    | 2019 | 1.5 billion   | Web text                              | Transformer  | 70.0% (LM)              | Improved Transformer model                | Advanced autoregressive capabilities, better text generation. |
| RoBERTa  | 2019 | 355 million   | Web text, Books, Wikipedia, CC-News   | Transformer  | 88.5% (LM), 97.5% (LM + FT) | Improved BERT model                      | Enhanced training and fine-tuning techniques for higher accuracy. |
| XLNet    | 2019 | 340 million   | Books, Wikipedia                      | Transformer  | 96.4% (LM)              | Improved Transformer model                | Combined best of BERT and GPT, leading in benchmark scores. |
| DistilBERT | 2019 | 66 million  | Books, Wikipedia                      | Transformer  | 83.8% (LM), 92.4% (LM + FT) | Lighter BERT model                      | Stripped-down version, maintaining performance with fewer parameters. |
| ALBERT   | 2019 | 11 million - 1.5 billion | Books, Wikipedia, CC-News, OpenWebText, Pile | Transformer | 89.4% (LM), 96.4% (LM + FT) | Parameter-efficient BERT                | Scaled BERT with parameter reduction and sharing for large-scale applications. |
| GPT-3    | 2020 | 175 billion  | Web text, Books, Wikipedia             | Transformer  | 77.9% (LM), 97.8% (LM + FT) | One of the largest models, capable of few-shot learning | Set new standards for size and few-shot learning abilities. |
| DeBERTa  | 2020 | 340 million  | Books, Wikipedia, CC-News, OpenWebText | Transformer  | 90.7% (LM), 96.7% (LM + FT) | Enhanced BERT with cross-layer parameter sharing | Introduced disentangled attention mechanism, refined BERT's architecture. |
| T5                | 2020 | 11 billion              | Web text, Books, Wikipedia, CC-News, TBC         | Transformer         | 89.8% (LM), 96.0% (LM + FT) | Text-to-Text Transfer Transformer                   | Unified framework transforming all NLP tasks into text-to-text format. |
| ELECTRA           | 2020 | 110 million - 1.1 billion | Books, Wikipedia, Common Crawl                  | Transformer         | 91.0% (LM), 96.3% (LM + FT) | Trained on discriminative loss                       | Efficiently learns representations by replacing tokens instead of predicting. |
| GShard            | 2020 | 600 billion             | Books, Wikipedia, News, Reddit, Arxiv            | Transformer         | 92.4% (LM), 98.6% (LM + FT) | Designed for massive-scale parallelism              | High performance on translation tasks, supports extremely large models. |
| Switch Transformer | 2021 | 1.6 trillion            | Books, Wikipedia, Common Crawl                  | Transformer         | 91.3% (LM), 97.1% (LM + FT) | Modular architecture for scalability                | Improves on GShard with sparsity for efficient training of very large models. |
| Codex             | 2021 | 6 billion               | Stack Overflow, GitHub                           | Transformer         | N/A                        | Trained on code and natural language for programming tasks | Specialized in interpreting and generating computer code. |
| GShard v3         | 2021 | 1.3 trillion            | Web text, Books, Wikipedia, Common Crawl, Pile   | Transformer         | 94.3% (LM), 98.9% (LM + FT) | Scaled-up version of GShard                         | Improved performance and efficiency for larger and more complex models. |
| CLIP              | 2021 | 400 million             | ImageNet, JFT-300 M                              | Transformer         | 85.4% (ImageNet), 53.5% (COCO) | Trained for cross-modal tasks with natural language and images | Bridges vision and language, supports a variety of image-based tasks. |
| GShard v4         | 2021 | 1.8 trillion            | Web text, Books, Wikipedia, Common Crawl, Pile, Reddit | Transformer      | 94.7% (LM), 99.0% (LM + FT) | Scaled-up version of GShard                         | Further scaling of GShard, pushing the limits of model size and performance. |
| DALL·E            | 2021 | 12 billion              | Text and images                                  | Transformer + CNN  | N/A                        | Trained for image generation from textual prompts   | Innovatively creates images from textual descriptions, showing understanding of concepts. |
| GPT-Neo           | 2021 | 2.7 billion - 2.8 trillion | Web text, Books, Wikipedia, Common Crawl, Reddit, Pile | Transformer    | 69.3% (LM) - 97.2% (LM + FT) | Open-source alternative to GPT                     | Community-driven, open-source initiative, promotes accessibility of LLMs. |
| AdaGPT        | 2021 | 1.8 billion     | Web text, Books, Wikipedia, Common Crawl, GPT-3        | Transformer          | 94.1% (LM), 98.8% (LM + FT) | Adapts to task-specific distributions                | Tailors to specific tasks for improved performance on diverse datasets. |
| Wu Dao        | 2021 | 1.75 trillion   | Web text, Books, Wikipedia, Common Crawl, Pile, GitHub | Transformer          | 96.3% (LM), 99.3% (LM + FT) | Developed by China's National Supercomputer Center   | Chinese initiative, pushing boundaries of scale and performance. |
| GPT-J         | 2021 | 6 billion       | Web text, Books, Wikipedia, Common Crawl, Stack Exchange, ArXiv, PubMed, EuroParl, Open Subtitles, Freebase | Transformer       | N/A                        | Open-source and community-led development            | Accessible alternative to GPT-3, fostered by the community. |
| Claude        | 2021 | 52 billion      | Web text, books                                        | Transformer          | N/A                        | Desirable behaviour in conversations                 | Focused on conversational behavior and ethical responses. |
| GPT-3 10x     | 2022 | 1.75 trillion   | Web text, Books, Wikipedia, Common Crawl, Pile         | Transformer          | 78.0% (LM), 98.9% (LM + FT) | Scaled-up version of GPT-3                          | Enhanced capabilities with increased scale, improved performance. |
| DALL·E 2      | 2022 | 22 billion      | Text and images                                        | Transformer + CNN    | N/A                        | Scaled-up version of DALL·E                         | Advanced image generation with more nuanced understanding. |
| GShard v5     | 2022 | 3.8 trillion    | Web text, Books, Wikipedia, Common Crawl, Pile, Reddit, Arxiv | Transformer       | N/A                        | Scaled-up version of GShard                         | Massive scale model for even more complex task handling. |
| BLOOM         | 2022 | 176 billion     | 46 natural languages and 13 programming languages      | Transformer          | N/A                        | GPT-3 based multi-lingual corpus                    | Multi-lingual capabilities, supporting diverse languages and programming tasks. |
| AlexaTM       | 2022 | 20 billion      | Mixture of common crawl and Wikipedia data across 12 languages using denoising and casual language modelling tasks | Transformer       | N/A                        | Bidirectional sequence-to-sequence architecture      | Specialized in handling multiple languages with a focus on bidirectional understanding. |
| LLaMA         | 2023 | 65 billion      | 20 language corpus                                     | Transformer          | N/A                        | Non-commercial research centric approach             | Prioritizes research with a wide language scope and ethical considerations. |

Reference - https://www.sciencedirect.com/science/article/pii/S266734522300024X#tbl3
